
# Conversion Measurement API

Dec 14, 2020

Meet link: [https://meet.google.com/jnn-rhxv-nsy](https://meet.google.com/jnn-rhxv-nsy)


# Agenda



*   Scribe volunteer
*   Agenda curation
*   Announcements:
    *   Breaking changes in Chrome 89
        *   [Hex encoding → integers](https://github.com/WICG/conversion-measurement-api/issues/60)
        *   [Updating names](https://github.com/WICG/conversion-measurement-api/issues/57#issuecomment-743403963)
        *   [JSON reports](https://github.com/privacycg/private-click-measurement/issues/30)
*   [SPURFOWL](https://github.com/AdRoll/privacy/blob/main/SPURFOWL.md)
*   [COWBIRD](https://github.com/AdRoll/privacy/blob/main/COWBIRD.md)
*   [CVR-dependent noise](https://github.com/WICG/conversion-measurement-api/issues/84)
*   [Authentication with event noise](https://github.com/WICG/conversion-measurement-api/issues/84)
*   Mobile app to web conversions
*   Any other business


# Attendees — please sign yourself in!



1. Charlie Harrison (Google)
2. Erik Taubeneck (FB)
3. Erik Anderson (Microsoft)
4. Xianfang Wang (Microsoft)
5. Mehul Parsana (Microsoft)
6. Ben Savage (FB)
7. Benjamin Case (FB)
8. Mikko Juola (NextRoll)
9. Valentino Volonghi (NextRoll)
10. Henry Corrigan-Gibbs (MIT CSAIL)
11. Przemyslaw Iwanczak (RTB House)
12. Raj Belur (Amazon)
13. Kevin Liao (MIT)
14. Jonasz Pamuła (RTB House)
15. Basile Leparmentier (Criteo)
16. Andrew Knox (Facebook)
17. Michael Kleber (Google)
18. Andrew Pascoe (NextRoll)
19. Robert Stratton (Neustar)


# Notes



*   Scribe: Erik Anderson
*   Agenda and curation:
    *   Charlie: &lt;Agenda intro>. SPURFOWL and COWBIRD had some local DP discussion that needed more time.
    *   Ben Savage: love openness, our ads business is ads shown in a mobile app that convert on a mobile web site. You see the ad that is natively rendered, 3p cookies impact that as well. Haven’t talked about what, if anything, to do about that.
    *   Basile Leparmentier: +1, great if it’s covered.
*   Breaking changes announcement:
    *   Charlie: weren’t folks using origin trial as of 2 weeks ago. You can expect a little breakage when it ends in Chrome 89. Some of the changes we expect to make is… the metadata is encoded using a hex encoding, but we’re changing it to normal integer encoding; there was no need for something separate that only RPI uses. Also in discussion with Safari on aligning on name updates, something has been stable for a couple of weeks. Might be name changes we update sooner. The other one is changing the structure of the reports the browser uses for sending conversions to a JSON structure instead of what we use currently.
*   SPURFOWL and COWBIRD:
    *   Matt Wilson for COWBIRD, Mikko Juola for SPURFOWL.
    *   Mikko: for SPURFOWL, to recap-- the high level proposal is that we’ll have the browser store impression events, click events, any 1p data from the site into this write-only trail store except for sandboxed JS functions that compute in a privacy-preserving way. We can write whatever reporting we want via JS, and the service helps… After that meeting, we had some issues; one that I thought was maybe not a deal breaker re: the trail store. Let’s say I have the trail store, click events, impression at some point. Browser computes, sends to aggregation service, eventually we get back the report we want to see. The issue is if I want to send a report later, do we delete the data in the trail store. If we do not delete the data, then I could make multiple records and remove the noise; multiple reports could delete differential privacy. Very useful to keep the trails in the store but it goes against the other requirement to keep DP. Some issues for how to get around this, I was thinking JS code could keep control over if we keep the event or not. Maybe we keep the data until we get the conversion report. Or if you send the report, it will be more noisy.

        Want to do a bit more prototyping on SPURFOWL, want to see how serious the issues are, see if we can make this a powerful tool for different reports. Want it to be as powerful as possible for agency while preserving privacy. Want to make sure we balance it correctly and keep it useful. If any of you are interested in this approach, we’re looking for collaborators to figure out what tradeoffs we can make.

*   Queue:
    *   Charlie: I think the two mitigations you discussed for duplicate reports make sense. I think the biggest thing on our side to make sure that the whole system is privacy-preserving enough is to make sure we are capping the number of reports any piece of data can produce. As long as you can cap that and have a concrete bound, then you can do all of the things like tuning the noise for how much any one record impacts the results. If you can reduce that cap to 1, every conversion/impression can only generate one report is great but if it’s higher than that I don’t think it’s a deal breaker-- just needs to be bounded. If unbounded then you can do these averaging attacks to strip out the noise. \
 \
My question related to that is, if we were to have a bound like the number of reports you could generate for any one piece of data before it gets deleted from the browser, what would that look like? 2 or 20? That really informs whether these techniques work.
    *   Mikko: looking at my own company and what reports we have, maybe 20 but that may not be practical and we would need to work that out. But maybe limit how many reports per day and we can probably work something out.
    *   Charlie: there might be research in this direction as well. Privacy Budget research goes in this direction. Little bit of a different angle of looking at things. Typically what I’ve seen is DB in central curator model and then your multiple reports look more like multiple queries of data because curator holds data at event level. Some techniques in this model try to do things like trying to find overlaps in queries. You can satisfy more different reports with fewer queries by figuring out some efficient linear combination (or something like that) so you can describe more data using queries you’ve already run using caching infra. More you can do if the reports have a lot of overlap. If you have two reports that are identical, don’t query twice, just remember the report and send it off to whoever wants that data. Maybe after the meeting I can send some papers over; not sure if anyone on the call is familiar.
    *   Mikko: with JS code the idea is 3p could use it, could limit the reports and get the same data.
    *   Andrew Knox (FB): less general purpose the query engine is, easier it is to make inferences about how data overlaps. Understand wanting to give as much flexibility as possible in the downstream use cases. Complete freedom is general query engine, but worse privacy position. Five most useful functions would result in a much cleaner thing. Knowing ahead of time knowing what all of the queries will be is extremely helpful for designing it. Can do it the other way, but your query planner can do it such that you can get a better shake. Certainly are techniques and we’ve studied it with papers out there.
    *   Mikko: right now we propose arbitrary JS reports, but in this model we’d have some other computation model that is less powerful but privacy will be preserved more with this other model.
    *   Andrew: might be much less noise than in general query model. The general idea is that there are only so many things people do, aggregate, sequence, count, etc. Few of the things get you most of the functionality.
    *   Charlie: to add on to that, our existing aggregation service proposal is align those veins: group by, count, and sum. Aggregation service proposal does not describe a general query engine, much more limited. Two different ideas being discussed here: (1) sum/counts and (2) JS environment with ultimate freedom to generate a report. Both of those can live alongside each other well, SPURFOWL doesn’t necessarily need to be a general counting engine, but it could be an environment to generate reports with fixed queries like summing or histograms.
    *   Erik Taubeneck: before getting into the queries and whatnot, the proposal needs a lot more of a description of the privacy model and how we would preserve privacy. Arbitrary environment to run Js is not privacy-preserving at all, could run select all to generate report, could maybe be more powerful than 3p cookie (at least the same). Questions of how many queries we allow before figuring out how much privacy to add to these reports.
    *   Mikko: next version of the proposal, we’ll add more detail. At the end it will be difficult to encode the actual data in the trails store. Kind of gloss over it with aggregation service to make sure this can’t happen, but I’ll add details.
    *   Michael Kleber: I agree that some sort of restricted computational model will help for how we get data out of the store. Something simple like if the data store is a sequence of events that got written using the write-only API one at a time, the on-device computational model will have to consume them one at a time for what it will send to the aggregation service. If there was a filtering stage first, writing a JS function that consumes one of these written-only reports and says “yes or no, do I want to use this particular item to use this report” and if the filtering said “yes”, it gets used in the report that gets sent to the aggregation service. Could run queries many times against this datastore, but maybe one per advertiser without worrying about sending data that got filtered out or counting it for the bounding epsilon over time problem.
    *   Mikko: you’re saying at a high level I can be selective about what events I use for the report and give that power to the agency.
    *   Michael: you could already be selective about what events you put in, but if processing API has two stages (1) just returning yes or no to use the item and (2) arbitrary JS computation approved by the first stage, then any accounting we need to do for if data will be exfiltrated report, you can know a priori what contributes and what we need to worry about.
    *   Mikko: this could be pretty useful, anything that can help is perfect. Thanks!
    *   Michael: this is along the lines of what Erik suggested to simplify.
    *   Charlie: similar to FB previous proposal-- conversion filters proposal that Ben had filed against all of the conversion measurement repos before. Had talked about this in aggregate reporting setting, running aggregate reporting filtering first and it’s fine since they don’t contribute to final report. Can add link. \
 \
To respond to Erik about privacy properties of API-- I agree we should probably write more about what we think the privacy properties are, but from my perspective we are building an environment which potentially gets as much or more than existing 3p cookies, so we have to be extremely careful, but at the same time as long as we can be sure we limit the scope of what this environment can accomplish, I feel like there’s a chance we can make it safe. If it can only generate reports for an aggregation service but not touch the page or network, there might be something here. Similar to the fenced frame stuff with an isolated environment that can’t touch DOM or network, but potentially more isolated since you don’t even render anything.
    *   Erik: wasn’t suggesting it’s not possible, will be powerful if we can figure it out. We should write it down if we can figure it out.
    *   Charlie: similar to the fenced frame, isolated environment protected in some way, but agree we can spend more time fleshing out the ideas and make sure it’s all working.

COWBIRD:



*   Charlie: haven’t discussed before.
*   Matt Wilson: will share slides tomorrow at W3C meeting. At a high level, the idea is the browser acts as federated learning platform for ad tech companies to have a lot of control over their modeling and what they optimize for and what features go into it. The motivation is that in the MURRE proposal we were experimenting and observed models with a very small hash space, like 100KB performed surprisingly well. Conceivable you could put many of tehse 100KB models in the browser. A federated learning platform would work by an ad tech company providing 100KB of model data, a function to compute features given contextual info, browser event data, etc. And then the learning part of federated learning company where the ad tech companies learn gradient info. Ad tech company provides a function to determine labels given browser event history (something like SPURFOWL). Also the ad tech company would provide a function to compute the model gradient. \
 \
This gives ad tech companies a lot of control over modeling. Anything in the model store can be an observation. Any feature you can generate from trail store becomes usable. Much more usable than TERN/TURTLEDOVE proposals which have weird partial observations you need to stitch together. COWBIRD has everything in one place, can compute things in a more reasonable way. \
 \
As far as attacks go from a privacy-preserving perspective-- gradients can be sparse or dense, can program the values of the gradients themselves. Gradients wouldn’t go directly to ad tech companies, would have something like aggregation API, gradients could be aggregated or distorted in a variety of ways. Compared to MURRE probably more privacy-preserving. Can be distorted in a bunch of ways while still being useful-- just have to point in the right direction. Federated learning in the browser!
*   Queue:
    *   Joao Natali: we’re interested in this, it is tremendously useful and aligned with what we want for general measurement. First question: you mention in the proposal that the coordination will be on the advertiser web site. Advertiser web site will be have some of your DSP code in it-- get this model, ping us for new models, calculating gradients, etc. Lots of impressions/trails will not hit the web site; how do you handle that from a perspective of understanding unsuccessful impressions in terms of clicks? If I understood correctly, the browser coordinates with the DSP, but this coordination starts with visiting the advertiser web site, correct?
    *   Matthew: yes, the browser goes to advertiser web site, it asks to download some model data, and it starts the process.
    *   J\oao: to understand the data, you need to understand if my browser did the conversion. Label success in the model. Majority of instances will be you feed impressions to a browser, but it won’t interact with advertiser web site. How do you account for those? Not in the model?
    *   Matthew: upper funnel rather than retargeting?
    *   Joao: yes.
    *   Matthew: during interest group requests, you don’t have to write just stuff related to advertiser in question. Could add similar advertisers to the one in question. Could do some upper funnel stuff that way. On shoes.com I could also write shoeadvertiser2.com into the browser as an interest group into my model. The browser has never been there but could show ads for them.
    *   Joao: but couldn’t model on them, right?
    *   Matthew: you could. Model would be trained with ad data. Don’t necessarily need to do that, but would work for upper model case.
    *   Joao: I will open an issue to continue conversation.
    *   Joao: one other question for Charlie-- in COWBIRD there are calculations about how expansive it would be in the browser. Ecosystem of DSPs, measurement companies, and so forth. Any guide on upper limits for how much computation, storage, etc. can happen in the browser?
    *   Charlie: great question, I don’t have an amazing answer right now. Currently we don’t have a lot of limits on computation that ads can run in the browser except for most egregious case. We have heavy ads intervention in Chrome-- ads using a certain percent of CPU or network we start intervening on them. It’s possible if we implement some of these proposals, we might start bringing some ads into the egregious case. Can look at what chrome considers egregious right now-- what would it look like in two years if we did this proposal?
    *   John Delaney (Chrome team, worked on interventions): if introduced in a web-facing way, browsers are pretty open in general to letting web sites do what they want and figure it out. We hope new APIs won’t push you up against these limits. I can also see a world where these APIs are first-party, so Chrome handles the computation and not constrained by these limits. Chrome has many ways of controlling resource usage to make sure it’s not apparent to the user.
    *   Michael Kleber: Android already has some form of federated learning on the platform-- it does it something like once a day, overnight. When the device thinks it won’t damage the user experience in some way. If Chrome has native understanding of federated learning, it would probably do something similar.
    *   Ben Savage: want to discuss the same concept. What is the openness to this concept on the Chrome team? Limited resources on mobile devices, can’t have an infinite number of models running overnight. Limits based on percentage of time that each web site is active for the user to figure out budget allocation? Or model size limitation? Etc. Or non-starter?
    *   Charlie: not a non-starter, if it provides a lot of value to the ecosystem we’re willing to consider it. Lots of options here that are respectful to users’ devices. Even right now the TURTLEDOVE proposals and things like that are cases where you can run arbitrary JS that could be an intense ML model. Nothing stopping it from being something like that and that’s true for a lot of these proposals. SPURFOWL JS could be doing arbitrarily complex computation. Doesn’t feel that different from existing proposals, just these arbitrary functions are computing model stuff instead of something fixed.
    *   Ben: it seems like there’s two routes; on-device or on a server in an MPC like environment. My intuition says there’s more resources to be had on servers and a little easier to deploy code/make changes/improvements if it’s on a server. Presumably less centralization of control. To the extent it’s possible to do federated learning or ML in a privacy-preserving way on a server, I would prefer that to on-device.
    *   Charlie: I agree these are both options to consider. I wouldn’t say either is a silver bullet. Obviously server-side learning is complicated if you want to add MPC and things like that onto it. With the client you gain nice simplicity but also a lot of complication.
    *   Robert Stratton (Neustar): going back to the PELICAN proposal, we were not specific about if on a server or the browser. What gave us confidence was that we’d need to do FLoC and it has 90% of the DNA for the ML we’d need to do for measurement. When we go back to the FLoC PoC, data was available to Chrome and Chrome does the FLoCs. Does only Chrome have access to the data, or do we start to connect these longer term? Seem to share a lot of the same requirements.
    *   Charlie: question has merit…
    *   Michael Kleber: the FLoC proposal, the way we’re experimenting with it right now, is not about doing ML on device or federated learning. Even though it’s in the name (I apologize). It’s certainly possible that if we learned clustering without federated learning isn’t good enough, but first thing we tried that seemed to be appropriate for the task is device assignment to users is entirely local and no server involved. So I’m not sure it fits into the paradigm here. On the other hand, it does fit more into the SPURFOWL model of taking a bunch of events, compute something locally, and exfiltrate something from the local computation with appropriate privacy properties. FLoC is different from things here is that the browser will go to substantial effort to make sure a person’s resulting value will be sufficiently non-privacy-sensitive so that it’s okay to associate the value directly with the person. A JS API will tell you the FLoC of the person. Feels like a different use case from on-device learning.
    *   Basile: TURTLEDOVE spec allows arbitrary JS, but have concerns about actual feasibility of it. Criteo’s ML making the device half as quick, I believe you won’t let us do it. Our models today are 1GB or bigger. If we do it on-device, there will be a trade-off for anyone using it, so it’s important to know that some things will not be possible anymore. Spec allows anything, but it’s important to know there are limited resources. One reason we proposed SPARROW is because we are concerned about actual feasibility at scale.

CVR-dependent noise



*   Charlie: this is something we at Google were looking at. This is related to local differential privacy noise we discussed last week. Using randomized response, coin flipping trick to get even event-level data. We could randomly inject fake conversions and drop conversions based on a coin flip. \
 \
One thing that we’re concerned about at Google is how this kind of noise would affect certain kinds of traffic. A combination of local and central noise to tune this system-- general idea is if you can describe your data in high level aggregates like “every impression is associated with a campaign” … these high-level aggregates could be used to perform noise locally on the browser. Instead of coin flip, you randomly assign conversion metadata based on true distribution of the data with central DP noise applied. Kind of a weird privacy mechanism, but I want to highlight it as one thing we’re exploring to potentially make this local DP a little easier to handle if we want to explore this for future privacy enhancements of event-level data. The biggest thing here is that it can be used to add an additional privacy lever onto the API so you can tune this one lever from uniform randomized response to distribution randomized response; lose privacy but you can increase the flip probability for generating random data. Adding another parameterization to the event level data… Proposal has a short description of the idea. Not in literature we can find.
*   Ben: I think it sounds awesome and innovative. Can you be more specific about how it would work? Where does the aggregate come from?
*   Charlie: naive way is you have a server that does MPC (or not…) and it learns the distribution of your data. Learning similar to what an aggregate API would learn. Your clients doing randomized response mechanism do coin flip to lie about outcome. Instead of picking outcome with uniform randomness, it can query server, tell it the data it does have (e.g. my campaign) and server might do randomized response and pick the sample from the distribution back to the client, “say you converted with this cohort or this metadata.” Simple way of explaining, doesn’t mesh with other concepts since the server would learn about sensitive aggregates. But might extend it to have more server privacy.
*   Ben: tricky thing here is aggregate reporting API is stateless. The privacy model here is a little weird now. All sorts of random browsers could access the data. Does it depend on me making random calls at some interval? [other questions]
*   Charlie: you ask the right questions, we don’t have all of the answers. Are all of these cohorts available to all browsers which might be sensitive since a random browser could learn a distribution is very worrying; one way you could resolve this is it could be a sidecar, encrypted reports gets sent to stateless operation … maybe we could come up with something fancier that doesn’t require all of that. We like event level API because of lack of fancy server-side stuff, these ideas kind of challenge that. All still open questions. On the Chrome side I don’t want to maintain a server that needs to be highly online with all Chrome clients hitting it which is why we made original API stateless, but these are things we can iterate on. Please add comments to existing issue. Issue talks about authentication so might be a little busy so we could break it into two issues.
*   Erik: issue of auth-- I don’t think it’s an issue. Shop.example reporting conversion is arbitrary JS call, can’t even auth that. Event binding I think shop.example can still create fake conversions. Just use domain-level trust token. I can write it up.
*   Charlie: don’t know if I exactly agree. If shop.example is themalicious party I would agree. The biggest thing I’m struggling with is for the case where shop.example is honest but arbitrary is not shop.example, the domain-bound trust tokens for shop.example won’t provide you the same level of protection as event-level binding. Domain-bound trust tokens can only do so much, I feel like you won’t be very conservative about how you vend those out whereas I think it’s easier to make a binary decision about trusting an event or not versus a person or not. Maybe if we’re good about not distributing domain-bound tokens unless the person does something like a purchase.
*   Erik: flip side is you need trust tokens to exist to generate fake conversions.
*   Charlie: yep, that’s my biggest concern.
*   Charlie: out of time, thanks! I will schedule something probably for second week of January since we’re out on the 4th. Use the same issue to do scheduling.